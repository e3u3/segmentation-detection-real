{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE152B: Homework 3\n",
    "## Computing Resources\n",
    "Please read the README file of this repository for the instructions\n",
    "## Instructions\n",
    "1. Attempt all questions.\n",
    "2. Please comment all your code adequately.\n",
    "3. Include all relevant information such as text answers, output images in notebook.\n",
    "4. **Academic integrity:** The homework must be completed individually.\n",
    "\n",
    "5. **Submission instructions:**  \n",
    " (a) Submit the notebook and its PDF version on Gradescope.  \n",
    " (b) Rename your submission files as Lastname_Firstname.ipynb and Lastname_Firstname.pdf.  \n",
    " (c) Correctly select pages for each answer on Gradescope to allow proper grading.\n",
    "\n",
    "6. **Due date:** Thu, Jun 4, at 4pm PST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: UNet for Image Segmentation\n",
    "\n",
    "``ALL the scripts here should be run from ./Segmentation``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Check the codes in `Segmentation`.** In this homework, we have provided the training script, the evaluation code, 3 verisons of the UNet structures and some useful functions. You will be asked to test out different variations of network structures and report their performance on the test set. The provided codes include:\n",
    "    1. `test.py`: The file for evaluation. \n",
    "    2. `dataLoader.py`: The file to load the data for training and testing.  \n",
    "    3. `model.py`: The file for models. The residual block (`ResBlock`) and the code to load pretrained weights of `resnet18` are given as `loadPretrainedWeight`. The basic encoder and decoder (`encoder` and `decoder`), as well as the ones with dilation/SPP (`encoderDilation` and `decoderDilation`) are also given. \n",
    "    4. `colormap.mat`: The color map used to visualize segmentation results. \n",
    "    5. `utils.py`: The file for some useful functions. The `computeAccuracy` function computes the unnormalized confusion matrix of each batch of labels. The `save_label` function turns the label into an image using the given color map and saves the image at the assigned location. Also see `test.py` for how these two functions are being used. \n",
    "    6. `train.py`: The file of the training script. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The network structures.**  There are 3 versions of UNet structures. In all three versions,  the `resnet18` structure before average pooling and fully connected layer is be used as the building block for encoder. \n",
    "\n",
    "1. `Basic UNet`: Please see `encoder` and `decoder` class in `model.py`. The `encoder` comes from `resnet18` and the decoder consists of transpose convolutional layers and bilinear interpolation layers so that the final output will be of the same size as the image. Skip links are added to help the network recover more details. \n",
    "\n",
    "2. `UNet with dilation`: We modify the encoder to a dilated `resnet18` as described in Section 2 of [1]. We set the stride of the last 4 residual blocks to be 1 so that the highest level feature maps will be $4\\times 4$ times larger. To increase the receptive field, we set the dilation of residual blocks that are fourth and third from the end to be 2, while the dilation of the residual blocks that are first and second from the end are set to 4.  The new encoder and decoder are implemented under class `encoderDilation` and `decoderDilation`.\n",
    "\n",
    "3. `UNet with dilation and pyramid pooling`:  Based on the encoder-decoder structure with dilation, pyramid pooling layer is added after the last residual block of encoder.  The pyramid pooling layer has been implemented following [2]. Notice that after adding the pyramid layer, the number of channels of the output feature to the first transpose convolutional layer will change from 512 to 1024. The new encoder and decoder are implemented under classes `encoderDilation` and `decoderDilation` (isSpp = True), respectively.\n",
    "\n",
    "**How the networks are trained.** The networks are trained using 1464 images from the training set of PASCAL VOC 2012. \n",
    "\n",
    "For example to train with dilation and SPP the following script is used:\n",
    "\n",
    "```python\n",
    "python train.py --isDilation --isSpp --nepoch 200 --isPretrained True --experiment checkpoints/unet_original_zq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Answer the following questions:**\n",
    "    1. Describe the loss function that is used to train the semantic segmentation network. **[10 points]**\n",
    "    2. Evaluate the trained models of `Basic UNet`, `UNet with dilation`, `UNet with dilation and pyramid pooling` provided as checkpoints using the following commands. Draw a table to summarize quantitative performances of the 3 variations of the UNet structure. The table should include the mean IoU of 21 categories of objects and their average mean IoU.  The testing mean IoU of 21 categories of object will saved to `.npy` files following the last line of `test.py`. You can make changes to `test.py` if necessary. **[10 points]** :\n",
    "        1. `Basic UNet`: You should load the pre-trained model with --modelRoot.\n",
    "        \n",
    "    ```python\n",
    "    python test.py --experiment checkpoints/unet_original_zq --modelRoot /datasets/cse152-252-sp20-public/unet_checkpoints/unet_original_zq --epochId 181\n",
    "    ```\n",
    "\n",
    "        2. `UNet with dilation`: \n",
    "        \n",
    "    ```python\n",
    "    python test.py --experiment checkpoints/unet_original_zq_dilation --isDilation --modelRoot /datasets/cse152-252-sp20-public/unet_checkpoints/unet_original_zq --epochId 180\n",
    "    \n",
    "    ```\n",
    "        3. `UNet with dilation and pyramid pooling`: \n",
    "        \n",
    "    ```python\n",
    "    python test.py --experiment checkpoints/unet_original_zq_spp --isSpp --modelRoot /datasets/cse152-252-sp20-public/unet_checkpoints/unet_original_zq --epochId 180\n",
    "            \n",
    "    ```\n",
    "    5. Make a figure for qualitative comparisons of the 3 methods, shown on 4 different input images. Please show the segmentation results for the same image but different networks so the differences can be compared. Briefly describe the results you obtain and any observations. **[10 points]** \n",
    "    6. Explain your observations in terms of: (i) what choices helped improve the accuracy and (ii) what other steps could have been tried to further improve accuracy?  **[10 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q1.2.A here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q1.2.B here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q1.2.C here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q1.2.D here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: SSD [3] Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Check the codes in `Detection`.** The codes are modified from ``https://github.com/amdegroot/ssd.pytorch``. Run `eval.py` code to get the object detection average precision (AP) on the PASCAL VOC 2012 dataset. The model is already trained on the PASCAL VOC 2012 object detection dataset. Draw a table in your report summarizing the AP of all 20 object categories and their mean.   **[10 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q2.1 here`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Answer the following questions:**\n",
    "    1.  Briefly explain how average precision is computed for PASCAL VOC 2012 dataset. Please check the code ($\\mathtt{eval.py:~Line~163-191}$) since there are different ways to compute average precision. **[10 points]** \n",
    "    2. Explain how SSD can be much faster compared to Faster RCNN [4]? **[10 points]**\n",
    "    3. Usually the number of negative bounding boxes (boxes without any object) is much larger than the number of positive bounding boxes. Explain how this imbalance is handled in SSD and Faster RCNN, respectively. **[10 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q2.2.A here`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q2.2.B here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q2.2.C here`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Randomly pick up some images from the PASCAL VOC 2012 dataset and some from other sources. Visualize the bounding box prediction results and include a figure in your report. You can use the code in folder $\\mathtt{demo}$ for visualization. **[10 points]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Answer Q2.3 here`` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Yu, Fisher, and Vladlen Koltun. \"Multi-scale context aggregation by dilated convolutions.\" arXiv preprint arXiv:1511.07122 (2015).\n",
    "2. Zhao, Hengshuang, et al. \"Pyramid scene parsing network.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n",
    "3. Liu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016.\n",
    "4.  Ren, Shaoqing, et al. \"Faster r-cnn: Towards real-time object detection with region proposal networks.\" Advances in neural information processing systems. 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
